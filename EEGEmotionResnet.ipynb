{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "YpJMj3awj00x",
        "EivtfviriTKp",
        "7vixDOxgjANf",
        "bjrld4YejVen",
        "5acIIOvqja_y",
        "CXRl1Lg_I-vJ"
      ],
      "mount_file_id": "1OmHPI_6IUmrTnGl84YnrmpfLcIxi9N1S",
      "authorship_tag": "ABX9TyOujEp6PjTLIhfYM6rZwltq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLNinja/EmotionResNET/blob/main/EEGEmotionResnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASbiDgcbimCL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATASET_PATH_STFT = \"/content/drive/MyDrive/Datasets/SEED/de_stft.npz\"\n",
        "DATASET_PATH_BANDPASS = \"/content/drive/MyDrive/Datasets/SEED/de_bandpass.npz\"\n",
        "DATASET_PATH_STFT_SMOOTH = \"/content/drive/MyDrive/Datasets/SEED/de_stft_smooth.npz\"\n",
        "DATASET_PATH_BANDPASS_SMOOTH = \"/content/drive/MyDrive/Datasets/SEED/de_bandpass_smooth.npz\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Model classes"
      ],
      "metadata": {
        "id": "CAduGuQPiMpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "class SEEDDataset(Dataset):\n",
        "    def __init__(self, npz_path, person_ids=None, transform=None):\n",
        "        data = np.load(npz_path)\n",
        "        X = data['X']\n",
        "        y = data['y']\n",
        "        persons = data['persons']\n",
        "        sessions = data['sessions']\n",
        "\n",
        "        if person_ids is not None:\n",
        "            mask = np.isin(persons, person_ids)\n",
        "            X = X[mask]\n",
        "            y = y[mask]\n",
        "            persons = persons[mask]\n",
        "            sessions = sessions[mask]\n",
        "\n",
        "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "        self.persons = torch.tensor(persons, dtype=torch.long)\n",
        "        self.sessions = torch.tensor(sessions, dtype=torch.long)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y, self.persons[idx], self.sessions[idx]"
      ],
      "metadata": {
        "id": "9I3hTQOLjCG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_persons = np.arange(0, 13)\n",
        "test_persons = np.arange(13, 15)\n",
        "\n",
        "train_dataset = SEEDDataset(DATASET_PATH_STFT_SMOOTH, person_ids=train_persons)\n",
        "test_dataset = SEEDDataset(DATASET_PATH_STFT_SMOOTH, person_ids=test_persons)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzsjXpROjCAE",
        "outputId": "26446b2e-6df2-441e-bcec-07c3c3250246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 131599\n",
            "Test samples: 20246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class EEGResNet(nn.Module):\n",
        "    def __init__(self, num_classes=4, pretrained=True):\n",
        "        super(EEGResNet, self).__init__()\n",
        "\n",
        "        self.backbone = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        self.backbone.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=64,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.backbone.maxpool = nn.Identity()\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n"
      ],
      "metadata": {
        "id": "jHoW58EJjB6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training process"
      ],
      "metadata": {
        "id": "bYkgsZI8iRPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subject-Dependent Training\n",
        "Model is trained on data from all persons, and from each session"
      ],
      "metadata": {
        "id": "YpJMj3awj00x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = np.load(DATASET_PATH_BANDPASS_SMOOTH)\n",
        "\n",
        "y = full_data[\"y\"]\n",
        "persons = full_data[\"persons\"]\n",
        "sessions = full_data[\"sessions\"]\n",
        "\n",
        "N = len(y)\n",
        "indices = np.arange(N)\n",
        "\n",
        "# Create joint stratification key\n",
        "# Each unique (person, session, label) gets preserved\n",
        "stratify_key = np.array([\n",
        "    f\"{p}_{s}_{l}\" for p, s, l in zip(persons, sessions, y)\n",
        "])\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.20,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=stratify_key\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_idx)}\")\n",
        "print(f\"Test  size: {len(test_idx)}\")\n",
        "\n",
        "full_dataset = SEEDDataset(DATASET_PATH_BANDPASS_SMOOTH, person_ids=None)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "test_dataset  = Subset(full_dataset, test_idx)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "63uje_O8j_G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EEGResNet(num_classes=num_classes, pretrained=False).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-2\n",
        ")\n",
        "\n",
        "metrics = {\n",
        "    \"train_acc\": [],\n",
        "    \"train_loss\": [],\n",
        "    \"val_acc\": [],\n",
        "    \"val_loss\": []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # -------- TRAIN --------\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for x, y, _, _ in loop:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        _, pred = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += pred.eq(y).sum().item()\n",
        "\n",
        "        loop.set_postfix(\n",
        "            loss=train_loss / total,\n",
        "            acc=100 * correct / total\n",
        "        )\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    # -------- VALIDATION --------\n",
        "    model.eval()\n",
        "    val_loss, cor, tot = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, _, _ in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            val_loss += loss.item() * x.size(0)\n",
        "            _, pred = out.max(1)\n",
        "            tot += y.size(0)\n",
        "            cor += pred.eq(y).sum().item()\n",
        "\n",
        "    val_acc = 100 * cor / tot\n",
        "    val_loss /= tot\n",
        "\n",
        "    metrics[\"train_acc\"].append(train_acc)\n",
        "    metrics[\"train_loss\"].append(train_loss)\n",
        "    metrics[\"val_acc\"].append(val_acc)\n",
        "    metrics[\"val_loss\"].append(val_loss)\n",
        "\n",
        "    np.save(METRICS_SAVE_PATH, metrics)\n",
        "\n",
        "    print(\n",
        "        f\"Train Acc: {train_acc:.2f}% | \"\n",
        "        f\"Val Acc: {val_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "    # -------- Save best model --------\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"val_acc\": val_acc\n",
        "            },\n",
        "            BEST_MODEL_PATH\n",
        "        )\n",
        "        print(f\"Best model saved (Val Acc = {val_acc:.2f}%)\")"
      ],
      "metadata": {
        "id": "PvQEjpAbkeKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leave One Subject Out (LOSO)\n",
        "Model trained on 13 persons, then tested on two persons it hasn't seen"
      ],
      "metadata": {
        "id": "EivtfviriTKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_persons = [0, 1]     # Persons evaluated in parallel\n",
        "metrics = {p: {\"val_acc\": [], \"val_loss\": []} for p in test_persons}\n",
        "METRICS_SAVE_PATH = \"/content/drive/MyDrive/Datasets/SEED/ResnetMetrics/two_person_metrics_stft.npz\"\n",
        "\n",
        "full_data = np.load(DATASET_PATH_STFT_SMOOTH)\n",
        "unique_persons = np.unique(full_data[\"persons\"])\n",
        "\n",
        "train_persons = [p for p in unique_persons if p not in test_persons]\n",
        "\n",
        "train_dataset = SEEDDataset(DATASET_PATH_STFT_SMOOTH, person_ids=train_persons)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_datasets = {\n",
        "    p: DataLoader(\n",
        "        SEEDDataset(DATASET_PATH_STFT_SMOOTH, person_ids=[p]),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "    for p in test_persons\n",
        "}\n",
        "\n",
        "model = EEGResNet(num_classes=num_classes, pretrained=False).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # -------- TRAIN --------\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    loop = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for x, y, _, _ in loop:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        _, pred = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += pred.eq(y).sum().item()\n",
        "\n",
        "        loop.set_postfix(loss=train_loss/total, acc=100*correct/total)\n",
        "\n",
        "    print(f\"Train Acc = {100*correct/total:.2f}%  |  Loss = {train_loss/total:.4f}\")\n",
        "\n",
        "    # -------- VALIDATION --------\n",
        "    model.eval()\n",
        "\n",
        "    for person_id in test_persons:\n",
        "        tot, cor, val_loss = 0, 0, 0.0\n",
        "        loader = test_datasets[person_id]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, _, _ in loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                val_loss += loss.item() * x.size(0)\n",
        "                _, pred = out.max(1)\n",
        "                tot += y.size(0)\n",
        "                cor += pred.eq(y).sum().item()\n",
        "\n",
        "        person_acc = 100 * cor / tot\n",
        "        person_loss = val_loss / tot\n",
        "\n",
        "        metrics[person_id][\"val_acc\"].append(person_acc)\n",
        "        metrics[person_id][\"val_loss\"].append(person_loss)\n",
        "\n",
        "        print(f\" Person {person_id} → Val Acc: {person_acc:.2f}% | Val Loss: {person_loss:.4f}\")\n",
        "\n",
        "    np.save(METRICS_SAVE_PATH, metrics)\n",
        "\n",
        "print(\"\\nTraining Finished!\")\n",
        "print(\"Final Metrics:\", metrics)\n"
      ],
      "metadata": {
        "id": "0GNAYmjE7Gyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## %5 test Person Data\n",
        "Model trains on 13 subjects and 5% from the two test persons"
      ],
      "metadata": {
        "id": "7vixDOxgjANf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_persons = [0, 1]\n",
        "metrics = {p: {\"val_acc\": [], \"val_loss\": []} for p in test_persons}\n",
        "\n",
        "METRICS_SAVE_PATH = \"/content/drive/MyDrive/Datasets/SEED/two_person_5_training_metrics.npz\"\n",
        "\n",
        "full_data = np.load(DATASET_PATH_STFT_SMOOTH)\n",
        "persons_arr = full_data[\"persons\"]\n",
        "unique_persons = np.unique(persons_arr)\n",
        "\n",
        "train_indices = []\n",
        "test_indices = []\n",
        "\n",
        "for person_id in test_persons:\n",
        "    idx = np.where(persons_arr == person_id)[0]\n",
        "\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=0.95,    # 95% into test\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_indices.extend(train_idx)\n",
        "    test_indices.extend(test_idx)\n",
        "\n",
        "other_persons = [p for p in unique_persons if p not in test_persons]\n",
        "\n",
        "for p in other_persons:\n",
        "    idx = np.where(persons_arr == p)[0]\n",
        "    train_indices.extend(idx)\n",
        "\n",
        "train_indices = np.array(train_indices)\n",
        "test_indices = np.array(test_indices)\n",
        "\n",
        "print(f\"Train samples from all persons: {len(train_indices)}\")\n",
        "print(f\"Test  samples from p0+p1 (95%): {len(test_indices)}\")\n",
        "\n",
        "full_dataset = SEEDDataset(DATASET_PATH_STFT_SMOOTH, person_ids=None)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "test_dataset_p0 = Subset(full_dataset, np.where((persons_arr == 0) & np.isin(np.arange(len(persons_arr)), test_indices))[0])\n",
        "test_dataset_p1 = Subset(full_dataset, np.where((persons_arr == 1) & np.isin(np.arange(len(persons_arr)), test_indices))[0])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loaders = {\n",
        "    0: DataLoader(test_dataset_p0, batch_size=batch_size, shuffle=False),\n",
        "    1: DataLoader(test_dataset_p1, batch_size=batch_size, shuffle=False)\n",
        "}\n",
        "\n",
        "model = EEGResNet(num_classes=num_classes, pretrained=False).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # -------- TRAIN ----------\n",
        "    model.train()\n",
        "    train_loss, total, correct = 0.0, 0, 0\n",
        "    loop = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for x, y, _, _ in loop:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        _, pred = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += pred.eq(y).sum().item()\n",
        "\n",
        "        loop.set_postfix(loss=train_loss/total, acc=100*correct/total)\n",
        "\n",
        "    print(f\"Train Acc = {100*correct/total:.2f}%  |  Loss = {train_loss/total:.4f}\")\n",
        "\n",
        "    # -------- VALIDATION (person 0 and person 1 separately) ----------\n",
        "    model.eval()\n",
        "\n",
        "    for pid in test_persons:\n",
        "        loader = test_loaders[pid]\n",
        "        tot, cor, vloss = 0, 0, 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, _, _ in loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                vloss += loss.item() * x.size(0)\n",
        "                _, pred = out.max(1)\n",
        "                tot += y.size(0)\n",
        "                cor += pred.eq(y).sum().item()\n",
        "\n",
        "        acc = 100 * cor / tot\n",
        "        loss_val = vloss / tot\n",
        "\n",
        "        metrics[pid][\"val_acc\"].append(acc)\n",
        "        metrics[pid][\"val_loss\"].append(loss_val)\n",
        "\n",
        "        print(f\" Person {pid}: Val Acc = {acc:.2f}% | Loss = {loss_val:.4f}\")\n",
        "\n",
        "    np.save(METRICS_SAVE_PATH, metrics)\n",
        "\n",
        "print(\"\\nTraining done!\")\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "id": "NDvXVvuvjMaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## %10 test Person Data\n",
        "Model trains on 13 subjects and 10% from the two test persons"
      ],
      "metadata": {
        "id": "bjrld4YejVen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_persons = [0, 1]\n",
        "metrics = {p: {\"val_acc\": [], \"val_loss\": []} for p in test_persons}\n",
        "\n",
        "METRICS_SAVE_PATH = \"/content/drive/MyDrive/Datasets/SEED/two_person_10_training_metrics.npz\"\n",
        "\n",
        "full_data = np.load(DATASET_PATH_STFT_SMOOTH)\n",
        "persons_arr = full_data[\"persons\"]\n",
        "unique_persons = np.unique(persons_arr)\n",
        "\n",
        "train_indices = []\n",
        "test_indices = []\n",
        "\n",
        "for person_id in test_persons:\n",
        "    idx = np.where(persons_arr == person_id)[0]\n",
        "\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=0.90,    # 90% into test\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_indices.extend(train_idx)\n",
        "    test_indices.extend(test_idx)\n",
        "\n",
        "other_persons = [p for p in unique_persons if p not in test_persons]\n",
        "\n",
        "for p in other_persons:\n",
        "    idx = np.where(persons_arr == p)[0]\n",
        "    train_indices.extend(idx)\n",
        "\n",
        "train_indices = np.array(train_indices)\n",
        "test_indices = np.array(test_indices)\n",
        "\n",
        "print(f\"Train samples from all persons: {len(train_indices)}\")\n",
        "print(f\"Test  samples from p0+p1 (90%): {len(test_indices)}\")\n",
        "\n",
        "full_dataset = SEEDDataset(DATASET_PATH_STFT_SMOOTH, person_ids=None)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "test_dataset_p0 = Subset(full_dataset, np.where((persons_arr == 0) & np.isin(np.arange(len(persons_arr)), test_indices))[0])\n",
        "test_dataset_p1 = Subset(full_dataset, np.where((persons_arr == 1) & np.isin(np.arange(len(persons_arr)), test_indices))[0])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loaders = {\n",
        "    0: DataLoader(test_dataset_p0, batch_size=batch_size, shuffle=False),\n",
        "    1: DataLoader(test_dataset_p1, batch_size=batch_size, shuffle=False)\n",
        "}\n",
        "\n",
        "model = EEGResNet(num_classes=num_classes, pretrained=False).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # -------- TRAIN ----------\n",
        "    model.train()\n",
        "    train_loss, total, correct = 0.0, 0, 0\n",
        "    loop = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for x, y, _, _ in loop:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        _, pred = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += pred.eq(y).sum().item()\n",
        "\n",
        "        loop.set_postfix(loss=train_loss/total, acc=100*correct/total)\n",
        "\n",
        "    print(f\"Train Acc = {100*correct/total:.2f}%  |  Loss = {train_loss/total:.4f}\")\n",
        "\n",
        "    # -------- VALIDATION (person 0 and person 1 separately) ----------\n",
        "    model.eval()\n",
        "\n",
        "    for pid in test_persons:\n",
        "        loader = test_loaders[pid]\n",
        "        tot, cor, vloss = 0, 0, 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, _, _ in loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                vloss += loss.item() * x.size(0)\n",
        "                _, pred = out.max(1)\n",
        "                tot += y.size(0)\n",
        "                cor += pred.eq(y).sum().item()\n",
        "\n",
        "        acc = 100 * cor / tot\n",
        "        loss_val = vloss / tot\n",
        "\n",
        "        metrics[pid][\"val_acc\"].append(acc)\n",
        "        metrics[pid][\"val_loss\"].append(loss_val)\n",
        "\n",
        "        print(f\" Person {pid}: Val Acc = {acc:.2f}% | Loss = {loss_val:.4f}\")\n",
        "\n",
        "    np.save(METRICS_SAVE_PATH, metrics)\n",
        "\n",
        "print(\"\\nTraining done!\")\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "MfMv3sNvjdWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## %15 test Person Data\n",
        "Model trains on 13 subjects and 15% from the two test persons"
      ],
      "metadata": {
        "id": "5acIIOvqja_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_persons = [0, 1]\n",
        "metrics = {p: {\"val_acc\": [], \"val_loss\": []} for p in test_persons}\n",
        "\n",
        "METRICS_SAVE_PATH = \"/content/drive/MyDrive/Datasets/SEED/two_person_15_training_metrics.npz\"\n",
        "\n",
        "full_data = np.load(DATASET_PATH_STFT_SMOOTH)\n",
        "persons_arr = full_data[\"persons\"]\n",
        "unique_persons = np.unique(persons_arr)\n",
        "\n",
        "train_indices = []\n",
        "test_indices = []\n",
        "\n",
        "for person_id in test_persons:\n",
        "    idx = np.where(persons_arr == person_id)[0]\n",
        "\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx,\n",
        "        test_size=0.85,    # 85% into test\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_indices.extend(train_idx)\n",
        "    test_indices.extend(test_idx)\n",
        "\n",
        "other_persons = [p for p in unique_persons if p not in test_persons]\n",
        "\n",
        "for p in other_persons:\n",
        "    idx = np.where(persons_arr == p)[0]\n",
        "    train_indices.extend(idx)\n",
        "\n",
        "train_indices = np.array(train_indices)\n",
        "test_indices = np.array(test_indices)\n",
        "\n",
        "print(f\"Train samples from all persons: {len(train_indices)}\")\n",
        "print(f\"Test  samples from p0+p1 (85%): {len(test_indices)}\")\n",
        "\n",
        "full_dataset = SEEDDataset(DATASET_PATH_STFT_SMOOTH, person_ids=None)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "test_dataset_p0 = Subset(full_dataset, np.where((persons_arr == 0) & np.isin(np.arange(len(persons_arr)), test_indices))[0])\n",
        "test_dataset_p1 = Subset(full_dataset, np.where((persons_arr == 1) & np.isin(np.arange(len(persons_arr)), test_indices))[0])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loaders = {\n",
        "    0: DataLoader(test_dataset_p0, batch_size=batch_size, shuffle=False),\n",
        "    1: DataLoader(test_dataset_p1, batch_size=batch_size, shuffle=False)\n",
        "}\n",
        "\n",
        "model = EEGResNet(num_classes=num_classes, pretrained=False).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # -------- TRAIN ----------\n",
        "    model.train()\n",
        "    train_loss, total, correct = 0.0, 0, 0\n",
        "    loop = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for x, y, _, _ in loop:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        _, pred = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += pred.eq(y).sum().item()\n",
        "\n",
        "        loop.set_postfix(loss=train_loss/total, acc=100*correct/total)\n",
        "\n",
        "    print(f\"Train Acc = {100*correct/total:.2f}%  |  Loss = {train_loss/total:.4f}\")\n",
        "\n",
        "    # -------- VALIDATION (person 0 and person 1 separately) ----------\n",
        "    model.eval()\n",
        "\n",
        "    for pid in test_persons:\n",
        "        loader = test_loaders[pid]\n",
        "        tot, cor, vloss = 0, 0, 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, _, _ in loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                vloss += loss.item() * x.size(0)\n",
        "                _, pred = out.max(1)\n",
        "                tot += y.size(0)\n",
        "                cor += pred.eq(y).sum().item()\n",
        "\n",
        "        acc = 100 * cor / tot\n",
        "        loss_val = vloss / tot\n",
        "\n",
        "        metrics[pid][\"val_acc\"].append(acc)\n",
        "        metrics[pid][\"val_loss\"].append(loss_val)\n",
        "\n",
        "        print(f\" Person {pid}: Val Acc = {acc:.2f}% | Loss = {loss_val:.4f}\")\n",
        "\n",
        "    np.save(METRICS_SAVE_PATH, metrics)\n",
        "\n",
        "print(\"\\nTraining done!\")\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "id": "a4NEfbI1jd1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Session based training"
      ],
      "metadata": {
        "id": "CXRl1Lg_I-vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = np.load(DATASET_PATH_STFT_SMOOTH)\n",
        "persons_arr  = full_data[\"persons\"]\n",
        "sessions_arr = full_data[\"sessions\"]\n",
        "\n",
        "TRAIN_SESSIONS = [0, 1]\n",
        "TEST_SESSION   = 2\n",
        "\n",
        "train_idx = np.where(np.isin(sessions_arr, TRAIN_SESSIONS))[0]\n",
        "test_idx  = np.where(sessions_arr == TEST_SESSION)[0]\n",
        "\n",
        "print(f\"Train samples (sessions 0,1): {len(train_idx)}\")\n",
        "print(f\"Test samples  (session 2)  : {len(test_idx)}\")\n",
        "\n",
        "test_persons = np.unique(persons_arr[test_idx]).tolist()\n",
        "print(\"Test persons:\", test_persons)"
      ],
      "metadata": {
        "id": "J5ENpJQZJAol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = SEEDDataset(DATASET_PATH_STFT_SMOOTH)\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "\n",
        "test_datasets = {\n",
        "    pid: Subset(\n",
        "        full_dataset,\n",
        "        test_idx[persons_arr[test_idx] == pid]\n",
        "    )\n",
        "    for pid in test_persons\n",
        "}\n",
        "\n",
        "for pid in test_persons:\n",
        "    print(f\"Person {pid} → test samples: {len(test_datasets[pid])}\")\n"
      ],
      "metadata": {
        "id": "CEGSREzWJrXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_loaders = {\n",
        "    pid: DataLoader(\n",
        "        test_datasets[pid],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "    for pid in test_persons\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    \"train\": {\n",
        "        \"acc\": [],\n",
        "        \"loss\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "# Add per-person validation metrics\n",
        "for pid in test_persons:\n",
        "    metrics[pid] = {\n",
        "        \"val_acc\": [],\n",
        "        \"val_loss\": []\n",
        "    }"
      ],
      "metadata": {
        "id": "h4xajwsNJ8o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS_SAVE_PATH = \"/content/drive/MyDrive/Datasets/SEED/session_based_metrics_stft.npz\"\n",
        "\n",
        "num_epochs = 50\n",
        "num_classes = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # -------- TRAIN --------\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for x, y, _, _ in loop:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        _, pred = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += pred.eq(y).sum().item()\n",
        "\n",
        "        loop.set_postfix(\n",
        "            loss=train_loss / total,\n",
        "            acc=100.0 * correct / total\n",
        "        )\n",
        "\n",
        "    train_acc = 100.0 * correct / total\n",
        "    train_loss_avg = train_loss / total\n",
        "\n",
        "    metrics[\"train\"][\"acc\"].append(train_acc)\n",
        "    metrics[\"train\"][\"loss\"].append(train_loss_avg)\n",
        "\n",
        "    print(f\"Train Acc = {train_acc:.2f}% | \"\n",
        "          f\"Loss = {train_loss_avg:.4f}\")\n",
        "\n",
        "    # -------- TEST (SESSION 2, ALL PERSONS) --------\n",
        "    model.eval()\n",
        "\n",
        "    for pid in test_persons:\n",
        "        loader = test_loaders[pid]\n",
        "        vloss, cor, tot = 0.0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, _, _ in loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "\n",
        "                out = model(x)\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                vloss += loss.item() * x.size(0)\n",
        "                _, pred = out.max(1)\n",
        "                tot += y.size(0)\n",
        "                cor += pred.eq(y).sum().item()\n",
        "\n",
        "        acc = 100.0 * cor / tot if tot > 0 else 0.0\n",
        "        loss_val = vloss / tot if tot > 0 else 0.0\n",
        "\n",
        "        metrics[pid][\"val_acc\"].append(acc)\n",
        "        metrics[pid][\"val_loss\"].append(loss_val)\n",
        "\n",
        "        print(f\" Person {pid:02d}: \"\n",
        "              f\"Acc = {acc:6.2f}% | \"\n",
        "              f\"Loss = {loss_val:.4f}\")\n",
        "\n",
        "    np.save(METRICS_SAVE_PATH, metrics)\n"
      ],
      "metadata": {
        "id": "34ijJ2LWKF_4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}